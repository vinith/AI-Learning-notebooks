{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "unable-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random as rand\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "x_digit = idx2numpy.convert_from_file('data/train-images-idx3-ubyte')\n",
    "y_digit = idx2numpy.convert_from_file('data/train-labels-idx1-ubyte')\n",
    "\n",
    "print(x_digit.shape, y_digit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "noble-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 56, 28) (30000, 2) (5000, 56, 28) (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "### Generate train and test data using mnist set with NUM_SIZE digits\n",
    "NUM_SIZE = 2\n",
    "\n",
    "image_dict = [[] for i in range(10)]\n",
    "for digit,image in zip(y_digit, x_digit):\n",
    "    image_dict[digit].append(image)\n",
    "\n",
    "\n",
    "def gen_test_data(num_data):\n",
    "    x_data, y_data = np.zeros((num_data, NUM_SIZE, 28, 28)), np.zeros((num_data, NUM_SIZE))\n",
    "    for i in range(num_data):\n",
    "        for k in range(NUM_SIZE):\n",
    "            rand_digit = rand.randint(0,9)\n",
    "            rand_img = rand.choice(image_dict[rand_digit])\n",
    "            y_data[i,k] = rand_digit\n",
    "            x_data[i,k] = rand_img\n",
    "    return x_data.reshape(num_data, NUM_SIZE * 28, -1), y_data\n",
    "\n",
    "x_train, y_train = gen_test_data(30000)\n",
    "x_test, y_test = gen_test_data(5000)\n",
    "x_test_check = x_test\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "verified-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1568, 30000) (20, 30000) (1568, 5000) (20, 5000)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.T.reshape(-1, 30000)\n",
    "x_test = x_test.T.reshape(-1,5000)\n",
    "y_test = tf.one_hot(y_test, 10).numpy().reshape(5000,-1).T\n",
    "y_train = tf.one_hot(y_train, 10).numpy().reshape(30000,-1).T\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "print(type(x_train), type(x_test), type(y_train), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "acting-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [x_train.shape[0], 512, 10 * NUM_SIZE]\n",
    "\n",
    "def init_params():\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        weights[l] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01;\n",
    "        biases[l] = np.zeros((layer_dims[l],1))\n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "leading-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "# 20 x m input\n",
    "def softmax(z):\n",
    "    return tf.nn.softmax(z.reshape(2,10,-1), axis=1).numpy().reshape(20,-1)\n",
    "    \n",
    "\n",
    "#  20 x m vs 20 x m\n",
    "def compute_cost(Y_expected, Y_actual):\n",
    "    #print(\"\\nExpected \\n\",Y_expected)\n",
    "    #print(\"\\n Actual \", Y_actual)\n",
    "    m = Y_expected.shape[1]\n",
    "    return -2/m * np.sum(np.multiply(Y_expected, np.log(Y_actual)))\n",
    "\n",
    "def forward_prop(X, W, B):\n",
    "    Z = {}\n",
    "    A = {0: X}\n",
    "    cost = 0\n",
    "    for l in range (1, len(layer_dims)):\n",
    "        Z[l] = np.dot(W[l] , A[l-1]) + B[l]\n",
    "        if l == len(layer_dims) -1:\n",
    "            A[l] = softmax(Z[l])\n",
    "        else:\n",
    "            A[l] = relu(Z[l])\n",
    "    return Z, A\n",
    "\n",
    "def calc_accuracy(Y_expected, Y_actual):\n",
    "    Y_actual = Y_actual.astype(int)\n",
    "    Y_expected = Y_expected.astype(int)\n",
    "    return 0.5 * np.sum(np.bitwise_and(Y_expected, Y_actual))/Y_expected.shape[1]\n",
    "\n",
    "def backward_prop_update_weights(Z, A, Y, W, B, learning_rate):\n",
    "    m = Y.shape[1]\n",
    "    #derivation flowing into the activation function (starts with 1 because dL/dL = 1)\n",
    "    propogated_derv = 1\n",
    "    activation_derv = 0\n",
    "    \n",
    "    for l in range(len(layer_dims) -1, 0 , -1):\n",
    "        if l == len(layer_dims)-1:\n",
    "            ##softmax derivative (Y_actual - Y_expected)\n",
    "            activation_derv = A[l] - Y\n",
    "        else:\n",
    "            ##relu derivative\n",
    "            activation_derv = (Z[l] > 0) * 1\n",
    "            \n",
    "        dZ = np.multiply(propogated_derv, activation_derv)\n",
    "        dW = 1/m * np.dot(dZ, A[l-1].T)\n",
    "        dB = 1/m * (np.sum(dZ,axis=1, keepdims=True))\n",
    "        W[l] = W[l] - learning_rate * dW\n",
    "        B[l] = B[l] - learning_rate * dB\n",
    "        propogated_derv = np.dot(W[l].T, dZ)\n",
    "    return W, B\n",
    "\n",
    "def train(X, Y, numitrs, learning_rate, batch_size):\n",
    "    W, B = init_params()\n",
    "    for itrs in range(numitrs):\n",
    "        print(itrs)\n",
    "        for batch in range(X.shape[1]//batch_size):\n",
    "            #print(\"Processing batch \" + str(batch*batch_size) + \" to \" + str((batch+1) * batch_size))\n",
    "            bx = X[:, batch*batch_size: (batch + 1) * batch_size]\n",
    "            by = Y[:, batch*batch_size: (batch + 1) * batch_size]\n",
    "            Z, A = forward_prop(bx, W, B)\n",
    "            W, B = backward_prop_update_weights(Z , A, by, W, B, learning_rate)\n",
    "        l = len(layer_dims) -1\n",
    "        cost = compute_cost(by, A[l])\n",
    "        accuracy = calc_accuracy(by, (A[l] > 0.5) * 1)\n",
    "        print(\"Cost :\" + str(cost) + \" Accuracy: \" + str(accuracy))\n",
    "    return W, B\n",
    "\n",
    "def predict(X, W, B):\n",
    "    Z, A = forward_prop(X, W, B)\n",
    "    return A[len(layer_dims)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "aggregate-seller",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cost :2.733306897955265 Accuracy: 0.8\n",
      "1\n",
      "Cost :2.1545981348508203 Accuracy: 0.845\n",
      "2\n",
      "Cost :1.8784244913646198 Accuracy: 0.865\n",
      "3\n",
      "Cost :1.6986661865135921 Accuracy: 0.885\n",
      "4\n",
      "Cost :1.5648919535086783 Accuracy: 0.895\n",
      "5\n",
      "Cost :1.464450872584493 Accuracy: 0.895\n",
      "6\n",
      "Cost :1.3847877189564222 Accuracy: 0.905\n",
      "7\n",
      "Cost :1.3156175748348913 Accuracy: 0.905\n",
      "8\n",
      "Cost :1.2541970064636803 Accuracy: 0.915\n",
      "9\n",
      "Cost :1.2005164949221732 Accuracy: 0.92\n",
      "10\n",
      "Cost :1.1518185491920874 Accuracy: 0.925\n",
      "11\n",
      "Cost :1.1078637434471572 Accuracy: 0.925\n",
      "12\n",
      "Cost :1.0675599468714336 Accuracy: 0.925\n",
      "13\n",
      "Cost :1.0321232366862676 Accuracy: 0.93\n",
      "14\n",
      "Cost :0.9993851864993175 Accuracy: 0.94\n",
      "15\n",
      "Cost :0.9687598752558313 Accuracy: 0.94\n",
      "16\n",
      "Cost :0.9392528323542395 Accuracy: 0.945\n",
      "17\n",
      "Cost :0.9097799103342787 Accuracy: 0.945\n",
      "18\n",
      "Cost :0.8816479364189738 Accuracy: 0.945\n",
      "19\n",
      "Cost :0.8551239727930442 Accuracy: 0.945\n",
      "20\n",
      "Cost :0.8303804092477962 Accuracy: 0.955\n",
      "21\n",
      "Cost :0.8061481862142239 Accuracy: 0.955\n",
      "22\n",
      "Cost :0.7829896040297064 Accuracy: 0.955\n",
      "23\n",
      "Cost :0.7608114808300104 Accuracy: 0.955\n",
      "24\n",
      "Cost :0.7395926948117498 Accuracy: 0.96\n",
      "25\n",
      "Cost :0.7184748206721429 Accuracy: 0.96\n",
      "26\n",
      "Cost :0.6997391760691233 Accuracy: 0.96\n",
      "27\n",
      "Cost :0.6796934033536418 Accuracy: 0.96\n",
      "28\n",
      "Cost :0.6606059274807495 Accuracy: 0.965\n",
      "29\n",
      "Cost :0.6422261402447806 Accuracy: 0.97\n",
      "30\n",
      "Cost :0.6246021198389026 Accuracy: 0.97\n",
      "31\n",
      "Cost :0.607302278081911 Accuracy: 0.97\n",
      "32\n",
      "Cost :0.5915568894864811 Accuracy: 0.97\n",
      "33\n",
      "Cost :0.5751356728166757 Accuracy: 0.97\n",
      "34\n",
      "Cost :0.5601347481729712 Accuracy: 0.97\n",
      "35\n",
      "Cost :0.5447092200370892 Accuracy: 0.97\n",
      "36\n",
      "Cost :0.530312848373134 Accuracy: 0.97\n",
      "37\n",
      "Cost :0.5166019469217686 Accuracy: 0.975\n",
      "38\n",
      "Cost :0.5038188859356528 Accuracy: 0.98\n",
      "39\n",
      "Cost :0.4906818554613937 Accuracy: 0.98\n",
      "40\n",
      "Cost :0.47851975850846873 Accuracy: 0.98\n",
      "41\n",
      "Cost :0.46640921283049713 Accuracy: 0.98\n",
      "42\n",
      "Cost :0.4546391991424876 Accuracy: 0.98\n",
      "43\n",
      "Cost :0.4434704668013751 Accuracy: 0.985\n",
      "44\n",
      "Cost :0.4327027216393522 Accuracy: 0.985\n",
      "45\n",
      "Cost :0.42197667302569936 Accuracy: 0.985\n",
      "46\n",
      "Cost :0.411727405076966 Accuracy: 0.985\n",
      "47\n",
      "Cost :0.40154285421770025 Accuracy: 0.985\n",
      "48\n",
      "Cost :0.39175791878587474 Accuracy: 0.985\n",
      "49\n",
      "Cost :0.38206201168896675 Accuracy: 0.985\n",
      "50\n",
      "Cost :0.3728923467368753 Accuracy: 0.985\n",
      "51\n",
      "Cost :0.36414111023503054 Accuracy: 0.985\n",
      "52\n",
      "Cost :0.35581265040901544 Accuracy: 0.985\n",
      "53\n",
      "Cost :0.34766040016418287 Accuracy: 0.985\n",
      "54\n",
      "Cost :0.3396221546437848 Accuracy: 0.985\n",
      "55\n",
      "Cost :0.33135517960589667 Accuracy: 0.985\n",
      "56\n",
      "Cost :0.32357590126244945 Accuracy: 0.985\n",
      "57\n",
      "Cost :0.3152893456617584 Accuracy: 0.985\n",
      "58\n",
      "Cost :0.30815553513007277 Accuracy: 0.985\n",
      "59\n",
      "Cost :0.3003105757136854 Accuracy: 0.985\n",
      "60\n",
      "Cost :0.2930778114814361 Accuracy: 0.985\n",
      "61\n",
      "Cost :0.2860287742239337 Accuracy: 0.99\n",
      "62\n",
      "Cost :0.27845375298070063 Accuracy: 0.99\n",
      "63\n",
      "Cost :0.2719983469049218 Accuracy: 0.99\n",
      "64\n",
      "Cost :0.26467485787201483 Accuracy: 0.99\n",
      "65\n",
      "Cost :0.2578517456353177 Accuracy: 0.99\n",
      "66\n",
      "Cost :0.2510963693185757 Accuracy: 0.99\n",
      "67\n",
      "Cost :0.2446136107577428 Accuracy: 0.99\n",
      "68\n",
      "Cost :0.23789744281519937 Accuracy: 0.99\n",
      "69\n",
      "Cost :0.23096818561104815 Accuracy: 0.99\n",
      "70\n",
      "Cost :0.22499859147623422 Accuracy: 0.99\n",
      "71\n",
      "Cost :0.21799588964839842 Accuracy: 0.99\n",
      "72\n",
      "Cost :0.21179887090714147 Accuracy: 0.99\n",
      "73\n",
      "Cost :0.20562375196306198 Accuracy: 0.99\n",
      "74\n",
      "Cost :0.19916525771533597 Accuracy: 0.99\n",
      "75\n",
      "Cost :0.1929339269885216 Accuracy: 0.99\n",
      "76\n",
      "Cost :0.18710613739674065 Accuracy: 0.99\n",
      "77\n",
      "Cost :0.18131699379796345 Accuracy: 0.99\n",
      "78\n",
      "Cost :0.17533274890615716 Accuracy: 0.99\n",
      "79\n",
      "Cost :0.16929333632801644 Accuracy: 0.99\n",
      "80\n",
      "Cost :0.16387719844838208 Accuracy: 0.99\n",
      "81\n",
      "Cost :0.1583123202472691 Accuracy: 0.99\n",
      "82\n",
      "Cost :0.1530315600765773 Accuracy: 0.99\n",
      "83\n",
      "Cost :0.14784954539905326 Accuracy: 0.99\n",
      "84\n",
      "Cost :0.1425934490269185 Accuracy: 0.99\n",
      "85\n",
      "Cost :0.13763663946329605 Accuracy: 0.99\n",
      "86\n",
      "Cost :0.1326296266852758 Accuracy: 0.99\n",
      "87\n",
      "Cost :0.1279157974682466 Accuracy: 0.99\n",
      "88\n",
      "Cost :0.12309312960342329 Accuracy: 0.99\n",
      "89\n",
      "Cost :0.11886031022153554 Accuracy: 0.99\n",
      "90\n",
      "Cost :0.11440776363629844 Accuracy: 0.99\n",
      "91\n",
      "Cost :0.11056906106536964 Accuracy: 0.99\n",
      "92\n",
      "Cost :0.10653922657728718 Accuracy: 0.99\n",
      "93\n",
      "Cost :0.10293498996782707 Accuracy: 0.99\n",
      "94\n",
      "Cost :0.09920945384882213 Accuracy: 0.99\n",
      "95\n",
      "Cost :0.09594789413592852 Accuracy: 0.995\n",
      "96\n",
      "Cost :0.09263459753772636 Accuracy: 0.995\n",
      "97\n",
      "Cost :0.08957884208048593 Accuracy: 0.995\n",
      "98\n",
      "Cost :0.0865855886417772 Accuracy: 0.995\n",
      "99\n",
      "Cost :0.08401779286762583 Accuracy: 0.995\n"
     ]
    }
   ],
   "source": [
    "W , B = train(x_train, y_train, 100, 0.0001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "auburn-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.9265\n"
     ]
    }
   ],
   "source": [
    "output = predict(x_test, W, B)\n",
    "print(\"Accuracy on test set is \" + str(calc_accuracy(y_test, (output > 0.5) * 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "geographic-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(5000, 56, 28)\n",
      "(56, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x30f9b10d0>"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print((output[:,100:101] > 0.5) * 1.0)\n",
    "test = x_test_check[100:101,:,:].reshape(56,-1)\n",
    "print(test.shape)\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-classroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38TF",
   "language": "python",
   "name": "p38tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
