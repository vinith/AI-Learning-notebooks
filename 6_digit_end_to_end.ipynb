{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "armed-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random as rand\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "x_digit = idx2numpy.convert_from_file('data/train-images-idx3-ubyte')\n",
    "y_digit = idx2numpy.convert_from_file('data/train-labels-idx1-ubyte')\n",
    "\n",
    "print(x_digit.shape, y_digit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "disciplinary-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 168, 28) (30000, 6) (2000, 168, 28) (2000, 6)\n"
     ]
    }
   ],
   "source": [
    "### Generate train and test data using mnist set with NUM_SIZE digits\n",
    "NUM_SIZE = 6\n",
    "\n",
    "image_dict = [[] for i in range(10)]\n",
    "for digit,image in zip(y_digit, x_digit):\n",
    "    image_dict[digit].append(image)\n",
    "\n",
    "\n",
    "def gen_test_data(num_data):\n",
    "    x_data, y_data = np.zeros((num_data, NUM_SIZE, 28, 28)), np.zeros((num_data, NUM_SIZE))\n",
    "    for i in range(num_data):\n",
    "        for k in range(NUM_SIZE):\n",
    "            rand_digit = rand.randint(0,9)\n",
    "            rand_img = rand.choice(image_dict[rand_digit])\n",
    "            y_data[i,k] = rand_digit\n",
    "            x_data[i,k] = rand_img\n",
    "    return x_data.reshape(num_data, NUM_SIZE * 28, -1), y_data\n",
    "\n",
    "x_train, y_train = gen_test_data(30000)\n",
    "x_test, y_test = gen_test_data(2000)\n",
    "x_test_check = x_test\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "juvenile-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4704, 30000) (60, 30000) (4704, 2000) (60, 2000)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.T.reshape(-1, 30000)\n",
    "x_test = x_test.T.reshape(-1,2000)\n",
    "y_test = tf.one_hot(y_test, 10).numpy().reshape(2000,-1).T\n",
    "y_train = tf.one_hot(y_train, 10).numpy().reshape(30000,-1).T\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "print(type(x_train), type(x_test), type(y_train), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "concrete-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [x_train.shape[0], 1024, 10 * NUM_SIZE]\n",
    "\n",
    "def init_params():\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        weights[l] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01;\n",
    "        biases[l] = np.zeros((layer_dims[l],1))\n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "generous-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "# 20 x m input\n",
    "def softmax(z):\n",
    "    return tf.nn.softmax(z.reshape(6,10,-1), axis=1).numpy().reshape(60,-1)\n",
    "    \n",
    "\n",
    "#  20 x m vs 20 x m\n",
    "def compute_cost(Y_expected, Y_actual):\n",
    "    #print(\"\\nExpected \\n\",Y_expected)\n",
    "    #print(\"\\n Actual \", Y_actual)\n",
    "    m = Y_expected.shape[1]\n",
    "    return -6/m * np.sum(np.multiply(Y_expected, np.log(Y_actual)))\n",
    "\n",
    "def forward_prop(X, W, B):\n",
    "    Z = {}\n",
    "    A = {0: X}\n",
    "    cost = 0\n",
    "    for l in range (1, len(layer_dims)):\n",
    "        Z[l] = np.dot(W[l] , A[l-1]) + B[l]\n",
    "        if l == len(layer_dims) -1:\n",
    "            A[l] = softmax(Z[l])\n",
    "        else:\n",
    "            A[l] = relu(Z[l])\n",
    "    return Z, A\n",
    "\n",
    "def calc_accuracy(Y_expected, Y_actual):\n",
    "    Y_actual = Y_actual.astype(int)\n",
    "    Y_expected = Y_expected.astype(int)\n",
    "    return (1.0/6.0) * np.sum(np.bitwise_and(Y_expected, Y_actual))/Y_expected.shape[1]\n",
    "\n",
    "def backward_prop_update_weights(Z, A, Y, W, B, learning_rate):\n",
    "    m = Y.shape[1]\n",
    "    #derivation flowing into the activation function (starts with 1 because dL/dL = 1)\n",
    "    propogated_derv = 1\n",
    "    activation_derv = 0\n",
    "    \n",
    "    for l in range(len(layer_dims) -1, 0 , -1):\n",
    "        if l == len(layer_dims)-1:\n",
    "            ##softmax derivative (Y_actual - Y_expected)\n",
    "            activation_derv = A[l] - Y\n",
    "        else:\n",
    "            ##relu derivative\n",
    "            activation_derv = (Z[l] > 0) * 1\n",
    "            \n",
    "        dZ = np.multiply(propogated_derv, activation_derv)\n",
    "        dW = 1/m * np.dot(dZ, A[l-1].T)\n",
    "        dB = 1/m * (np.sum(dZ,axis=1, keepdims=True))\n",
    "        W[l] = W[l] - learning_rate * dW\n",
    "        B[l] = B[l] - learning_rate * dB\n",
    "        propogated_derv = np.dot(W[l].T, dZ)\n",
    "    return W, B\n",
    "\n",
    "def train(X, Y, numitrs, learning_rate, batch_size):\n",
    "    W, B = init_params()\n",
    "    lr = 0\n",
    "    for itrs in range(numitrs):\n",
    "        print(itrs)\n",
    "        for batch in range(X.shape[1]//batch_size):\n",
    "            if itrs <=12: lr = 0.00007\n",
    "            else: lr = learning_rate\n",
    "            #print(\"Processing batch \" + str(batch*batch_size) + \" to \" + str((batch+1) * batch_size))\n",
    "            bx = X[:, batch*batch_size: (batch + 1) * batch_size]\n",
    "            by = Y[:, batch*batch_size: (batch + 1) * batch_size]\n",
    "            Z, A = forward_prop(bx, W, B)\n",
    "            W, B = backward_prop_update_weights(Z , A, by, W, B, lr)\n",
    "        l = len(layer_dims) -1\n",
    "        cost = compute_cost(by, A[l])\n",
    "        accuracy = calc_accuracy(by, (A[l] > 0.5) * 1)\n",
    "        print(\"Cost :\" + str(cost) + \" Accuracy: \" + str(accuracy) + \" lr = \" + str(lr))\n",
    "    return W, B\n",
    "\n",
    "def predict(X, W, B):\n",
    "    Z, A = forward_prop(X, W, B)\n",
    "    return A[len(layer_dims)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-commonwealth",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cost :211.61178252417292 Accuracy: 0.3118333333333333 lr = 7e-05\n",
      "1\n",
      "Cost :132.2748237231843 Accuracy: 0.4498333333333333 lr = 7e-05\n",
      "2\n",
      "Cost :100.15796760603223 Accuracy: 0.5331666666666667 lr = 7e-05\n",
      "3\n",
      "Cost :83.43479552373378 Accuracy: 0.5856666666666667 lr = 7e-05\n",
      "4\n",
      "Cost :72.5466730929213 Accuracy: 0.6221666666666666 lr = 7e-05\n",
      "5\n",
      "Cost :64.83054718757158 Accuracy: 0.6518333333333333 lr = 7e-05\n",
      "6\n",
      "Cost :58.9809805517025 Accuracy: 0.674 lr = 7e-05\n",
      "7\n",
      "Cost :54.40549115648135 Accuracy: 0.6908333333333333 lr = 7e-05\n",
      "8\n",
      "Cost :50.71733361475955 Accuracy: 0.7061666666666666 lr = 7e-05\n",
      "9\n",
      "Cost :47.71909895034713 Accuracy: 0.7208333333333332 lr = 7e-05\n",
      "10\n",
      "Cost :45.281441062813755 Accuracy: 0.7316666666666666 lr = 7e-05\n",
      "11\n",
      "Cost :43.24144793442301 Accuracy: 0.7418333333333332 lr = 7e-05\n",
      "12\n",
      "Cost :41.43537681103147 Accuracy: 0.749 lr = 7e-05\n",
      "13\n",
      "Cost :39.01527096426698 Accuracy: 0.7503333333333333 lr = 2e-05\n",
      "14\n",
      "Cost :38.54114351723982 Accuracy: 0.7526666666666666 lr = 2e-05\n",
      "15\n",
      "Cost :38.08176873562724 Accuracy: 0.7546666666666666 lr = 2e-05\n",
      "16\n",
      "Cost :37.63708841541227 Accuracy: 0.7568333333333332 lr = 2e-05\n",
      "17\n",
      "Cost :37.205666681267815 Accuracy: 0.7576666666666666 lr = 2e-05\n",
      "18\n",
      "Cost :36.78777658567247 Accuracy: 0.759 lr = 2e-05\n",
      "19\n",
      "Cost :36.381851749451414 Accuracy: 0.7613333333333333 lr = 2e-05\n",
      "20\n",
      "Cost :35.98790764601623 Accuracy: 0.763 lr = 2e-05\n",
      "21\n",
      "Cost :35.60522042866836 Accuracy: 0.7653333333333333 lr = 2e-05\n",
      "22\n",
      "Cost :35.23173880650069 Accuracy: 0.7666666666666666 lr = 2e-05\n",
      "23\n",
      "Cost :34.867807576661775 Accuracy: 0.7686666666666666 lr = 2e-05\n",
      "24\n",
      "Cost :34.51329172680482 Accuracy: 0.7698333333333333 lr = 2e-05\n",
      "25\n",
      "Cost :34.16792546903067 Accuracy: 0.7711666666666667 lr = 2e-05\n",
      "26\n",
      "Cost :33.83194124489882 Accuracy: 0.7733333333333332 lr = 2e-05\n",
      "27\n",
      "Cost :33.503896425520985 Accuracy: 0.7755 lr = 2e-05\n",
      "28\n",
      "Cost :33.184227272732926 Accuracy: 0.7771666666666667 lr = 2e-05\n",
      "29\n",
      "Cost :32.872846056376396 Accuracy: 0.7786666666666666 lr = 2e-05\n",
      "30\n",
      "Cost :32.56935267951412 Accuracy: 0.78 lr = 2e-05\n",
      "31\n",
      "Cost :32.27332082584367 Accuracy: 0.7815 lr = 2e-05\n",
      "32\n",
      "Cost :31.98385170826052 Accuracy: 0.7821666666666667 lr = 2e-05\n",
      "33\n",
      "Cost :31.700668645103775 Accuracy: 0.7828333333333333 lr = 2e-05\n",
      "34\n",
      "Cost :31.424093594715398 Accuracy: 0.7843333333333332 lr = 2e-05\n",
      "35\n",
      "Cost :31.15331006873905 Accuracy: 0.785 lr = 2e-05\n",
      "36\n",
      "Cost :30.88791346599739 Accuracy: 0.7868333333333333 lr = 2e-05\n",
      "37\n",
      "Cost :30.629482769939468 Accuracy: 0.7886666666666666 lr = 2e-05\n",
      "38\n",
      "Cost :30.376179880218448 Accuracy: 0.7898333333333333 lr = 2e-05\n",
      "39\n",
      "Cost :30.128366337846163 Accuracy: 0.792 lr = 2e-05\n",
      "40\n",
      "Cost :29.886113189461604 Accuracy: 0.7923333333333332 lr = 2e-05\n",
      "41\n",
      "Cost :29.648815101663924 Accuracy: 0.7936666666666666 lr = 2e-05\n",
      "42\n",
      "Cost :29.41621880056548 Accuracy: 0.7941666666666666 lr = 2e-05\n",
      "43\n",
      "Cost :29.188678276371697 Accuracy: 0.7946666666666666 lr = 2e-05\n",
      "44\n",
      "Cost :28.965871466031125 Accuracy: 0.7955 lr = 2e-05\n",
      "45\n",
      "Cost :28.74750451922936 Accuracy: 0.7968333333333333 lr = 2e-05\n",
      "46\n",
      "Cost :28.53393400609784 Accuracy: 0.7986666666666666 lr = 2e-05\n",
      "47\n",
      "Cost :28.324675137905263 Accuracy: 0.7996666666666666 lr = 2e-05\n",
      "48\n",
      "Cost :28.119442437216094 Accuracy: 0.8005 lr = 2e-05\n",
      "49\n",
      "Cost :27.918244813446755 Accuracy: 0.8018333333333333 lr = 2e-05\n",
      "50\n",
      "Cost :27.72083412082572 Accuracy: 0.8025 lr = 2e-05\n",
      "51\n",
      "Cost :27.52654370032449 Accuracy: 0.803 lr = 2e-05\n",
      "52\n",
      "Cost :27.336008075810536 Accuracy: 0.8038333333333333 lr = 2e-05\n",
      "53\n",
      "Cost :27.14909780473958 Accuracy: 0.805 lr = 2e-05\n",
      "54\n",
      "Cost :26.965734919938992 Accuracy: 0.8058333333333333 lr = 2e-05\n",
      "55\n",
      "Cost :26.785590810130465 Accuracy: 0.8071666666666666 lr = 2e-05\n",
      "56\n",
      "Cost :26.60879779633277 Accuracy: 0.808 lr = 2e-05\n",
      "57\n",
      "Cost :26.43481075738793 Accuracy: 0.8081666666666666 lr = 2e-05\n",
      "58\n",
      "Cost :26.26314769076133 Accuracy: 0.8093333333333332 lr = 2e-05\n",
      "59\n",
      "Cost :26.094269745483075 Accuracy: 0.8105 lr = 2e-05\n",
      "60\n",
      "Cost :25.92792735552201 Accuracy: 0.8118333333333333 lr = 2e-05\n",
      "61\n",
      "Cost :25.76423817116552 Accuracy: 0.812 lr = 2e-05\n",
      "62\n",
      "Cost :25.60309610665161 Accuracy: 0.8126666666666666 lr = 2e-05\n",
      "63\n",
      "Cost :25.444534744837668 Accuracy: 0.813 lr = 2e-05\n",
      "64\n",
      "Cost :25.288519340925458 Accuracy: 0.8133333333333332 lr = 2e-05\n",
      "65\n",
      "Cost :25.134904644858015 Accuracy: 0.814 lr = 2e-05\n",
      "66\n",
      "Cost :24.984030921232137 Accuracy: 0.8146666666666667 lr = 2e-05\n",
      "67\n",
      "Cost :24.83550535783782 Accuracy: 0.8158333333333333 lr = 2e-05\n",
      "68\n",
      "Cost :24.6892733555113 Accuracy: 0.8166666666666667 lr = 2e-05\n",
      "69\n",
      "Cost :24.545292215774996 Accuracy: 0.8176666666666667 lr = 2e-05\n",
      "70\n",
      "Cost :24.403611311703376 Accuracy: 0.8181666666666666 lr = 2e-05\n",
      "71\n",
      "Cost :24.263862836724503 Accuracy: 0.8193333333333332 lr = 2e-05\n",
      "72\n",
      "Cost :24.126374431429923 Accuracy: 0.8198333333333333 lr = 2e-05\n",
      "73\n",
      "Cost :23.991029056292252 Accuracy: 0.8208333333333333 lr = 2e-05\n",
      "74\n",
      "Cost :23.857571225156047 Accuracy: 0.8211666666666666 lr = 2e-05\n",
      "75\n",
      "Cost :23.725575086642174 Accuracy: 0.822 lr = 2e-05\n",
      "76\n",
      "Cost :23.595376144939383 Accuracy: 0.8233333333333333 lr = 2e-05\n",
      "77\n",
      "Cost :23.46727705253341 Accuracy: 0.8243333333333333 lr = 2e-05\n",
      "78\n",
      "Cost :23.34101137204551 Accuracy: 0.8251666666666666 lr = 2e-05\n",
      "79\n",
      "Cost :23.21618837385052 Accuracy: 0.8258333333333333 lr = 2e-05\n",
      "80\n",
      "Cost :23.092604296463687 Accuracy: 0.8271666666666666 lr = 2e-05\n",
      "81\n",
      "Cost :22.970711983129686 Accuracy: 0.8278333333333333 lr = 2e-05\n",
      "82\n",
      "Cost :22.850382049063903 Accuracy: 0.8281666666666666 lr = 2e-05\n",
      "83\n",
      "Cost :22.731564484469168 Accuracy: 0.8285 lr = 2e-05\n",
      "84\n",
      "Cost :22.614247730429973 Accuracy: 0.8286666666666667 lr = 2e-05\n",
      "85\n",
      "Cost :22.498515165077386 Accuracy: 0.8293333333333333 lr = 2e-05\n",
      "86\n",
      "Cost :22.384225558916302 Accuracy: 0.8298333333333333 lr = 2e-05\n",
      "87\n",
      "Cost :22.271188630604033 Accuracy: 0.83 lr = 2e-05\n",
      "88\n",
      "Cost :22.15954087506707 Accuracy: 0.8306666666666667 lr = 2e-05\n",
      "89\n",
      "Cost :22.04910880291665 Accuracy: 0.831 lr = 2e-05\n",
      "90\n",
      "Cost :21.940098756252052 Accuracy: 0.8315 lr = 2e-05\n",
      "91\n",
      "Cost :21.832270693992534 Accuracy: 0.8318333333333332 lr = 2e-05\n",
      "92\n",
      "Cost :21.725730785426617 Accuracy: 0.8321666666666666 lr = 2e-05\n",
      "93\n",
      "Cost :21.620689809781464 Accuracy: 0.833 lr = 2e-05\n",
      "94\n",
      "Cost :21.51674421171956 Accuracy: 0.8335 lr = 2e-05\n",
      "95\n",
      "Cost :21.414248605394615 Accuracy: 0.8336666666666667 lr = 2e-05\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "W , B = train(x_train, y_train, 100, 0.00002, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "incident-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.79825\n"
     ]
    }
   ],
   "source": [
    "output = predict(x_test, W, B)\n",
    "print(\"Accuracy on test set is \" + str(calc_accuracy(y_test, (output > 0.5) * 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "passive-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(5000, 56, 28)\n",
      "(56, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x30f9b10d0>"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print((output[:,100:101] > 0.5) * 1.0)\n",
    "test = x_test_check[100:101,:,:].reshape(56,-1)\n",
    "print(test.shape)\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-waterproof",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38TF",
   "language": "python",
   "name": "p38tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
